---
name: google-ads-ops
description: Use when the user wants to run Google Ads campaigns with arbitrage strategy, A/B test target groups and value propositions, or needs week-by-week campaign optimization with hypothesis-driven experiments.
---

# Google Ads Ops

## Overview
Run Google Ads campaigns with an **arbitrage mindset**: test hypotheses, measure Cost per Order (CpO), and iterate weekly. Focus on conversions, not vanity metrics. Each experiment tests ONE bold change so results are attributable.

## When to Use
- User mentions Google Ads, AdSense, or paid acquisition
- User wants to optimize conversion performance
- User has budget for experiments and wants measurable ROI
- User is starting from scratch OR has existing campaigns

## Phase 1: Socratic Discovery (Initialization)

**Goal:** Uncover the user's business model and arbitrage opportunity through questions.

### Opening Questions (ask 3-5)
1. **Business model:** "What do you sell and what does each conversion earn you?" (understand unit economics)
2. **Target audience:** "Who buys this? Describe your ideal customer." â†’ Follow up: "What do they care about most when making this decision?"
3. **Value proposition:** "Why do customers choose you over alternatives?" â†’ Follow up: "If you had to guess, what's the ONE thing that makes them convert?"
4. **Budget:** "What's your comfortable loss limit to learn what works?" (default: 30â‚¬ per experiment)
5. **Experience:** "Have you run ads before? What worked or didn't?" (assume 0, ask kindly)
6. **Target group size:** "How many potential customers exist for this? Local, national, global?"

### Transition to Execution
After 3-5 questions, **pause and offer**:

> "I have enough to propose initial experiments. We can proceed now, or I can ask 3 more questions to refine the strategy. What do you prefer?"

If user says proceed â†’ **Move to Phase 2**

### Exit Criteria for Discovery
- âœ… Unit economics understood (what you pay vs. what you earn per conversion)
- âœ… Target audience hypothesis defined
- âœ… Value proposition hypothesis defined
- âœ… Experiment budget confirmed
- âœ… User has chosen to proceed

## Phase 2: Hypothesis-Driven Execution

### Budget Splitting Rule
**30â‚¬ per experiment.** Each experiment tests ONE hypothesis.

```
Example budget split (150â‚¬ total):
â”œâ”€â”€ Experiment A: 30â‚¬ - Test audience segment "fashion-conscious millennials"
â”œâ”€â”€ Experiment B: 30â‚¬ - Test value proposition "Save 2 hours per week"
â”œâ”€â”€ Experiment C: 30â‚¬ - Test keyword cluster "personal color analysis"
â”œâ”€â”€ Experiment D: 30â‚¬ - Test landing page variant A
â””â”€â”€ Experiment E: 30â‚¬ - Test bidding strategy (maximize clicks vs conversions)
```

### The ONE Change Rule
**Each experiment changes exactly ONE variable.** Never mix changes.

| âŒ Bad (mixed changes) | âœ… Good (single change) |
|------------------------|-------------------------|
| New audience + new ad copy | New audience (same ad copy) |
| New keywords + higher bids | New keywords (same bids) |
| New landing page + new CTA | New landing page (same CTA) |

**Why:** If results change, we must know WHAT caused it.

### Campaign Proposal Format
Before booking, present to user:

```markdown
## Proposed Experiment: [Name]

**Hypothesis:** "[Audience/Value prop/Channel] will achieve [CpO target]"

**What we're testing:** [Single variable]
**Control (baseline):** [What we're comparing against]
**Budget:** 30â‚¬ over 7 days
**Target metrics:**
- Expected CpO: [X]â‚¬
- Target conversions: [Y]

**Campaign structure:**
- Audience: [segment]
- Keywords: [list]
- Ad copy: [variant]
- Landing page: [URL]

**Proceed? (yes/no/modify)**
```

User approval â†’ **Book campaign directly**

## Phase 3: Weekly Review Cycle

### Every 7 Days
**Proactively** generate morning report:

```markdown
## Week [N] Performance Report

### Summary
- Spend: [X]â‚¬
- Conversions: [Y]
- Cost per Order: [Z]â‚¬
- Customer journey duration: [D] days (analyzed from data)

### Hypothesis Results
| Experiment | Hypothesis | Result | Verdict |
|------------|------------|--------|---------|
| A | [description] | CpO: [X]â‚¬ | âœ… Scale / âŒ Kill / ğŸ”„ Iterate |

### What We Learned
- [Insight 1]
- [Insight 2]

### Next Week's Experiments
Based on learnings, propose 2-3 new experiments...

**Approve new experiments? (yes/no/discuss)**
```

### Customer Journey Analysis
After first conversions, analyze and report:
- **Time from first click to conversion** (from Analytics data)
- **Touchpoints before conversion** (search â†’ ad â†’ landing page â†’ convert)
- **Implication for budget timing:** "Conversions happen ~[X] days after click, so allocate budget for [X+2] day measurement window"

### No Automatic Adjustments
**Rule:** Agent does NOT automatically change budgets or bids.

Workflow:
1. Measure â†’ Learn â†’ Report â†’ Propose new hypotheses â†’ User approves â†’ Execute

## Phase 4: ROI Measurement

### Primary Metric: Cost per Order (CpO)
```
CpO = Total Spend / Number of Orders

Target: CpO < Customer Lifetime Value Ã— Profit Margin
```

### Secondary Metrics (for diagnostics)
| Metric | Use For |
|--------|---------|
| CTR | Ad relevance (low CTR â†’ test copy) |
| CPC | Competition level (high CPC â†’ test long-tail) |
| Conversion Rate | Landing page effectiveness |
| Quality Score | Keyword-ad relevance |

### Break-Even Calculation
Ask user: "What's a customer worth to you?" â†’ Set CpO target at 50-70% of that value for safety margin.

## Safety Rules

1. **Never store credentials in repo** - use env vars or local JSON only
2. **Always confirm before booking** campaigns, even small ones
3. **Never exceed stated experiment budget** without explicit approval
4. **If API access fails** - ask for credentials, don't guess
5. **If data is insufficient** - say so, don't fabricate insights

## Quick Reference

| Phase | Trigger | Action |
|-------|---------|--------|
| Discovery | User starts conversation | Ask 3-5 Socratic questions |
| Proposal | User ready to proceed | Present experiments, get approval |
| Execution | User approves | Book campaign, start 7-day clock |
| Review | Every 7 days (proactive) | Generate report, propose next experiments |

## Common Mistakes

| Mistake | Fix |
|---------|-----|
| Testing multiple variables at once | Enforce ONE change per experiment |
| Skipping discovery questions | Always complete 3-5 questions minimum |
| Adjusting mid-week | Let experiments run full 7 days |
| Ignoring customer journey timing | Analyze duration, adjust measurement window |
| Focusing on CTR instead of CpO | CTR is diagnostic, CpO is the goal |

## Example Dialogue

**User:** "I want to run Google Ads for my color analysis app."

**Agent (Discovery):**
1. "What does each conversion earn you? Is it a one-time purchase or subscription?"
2. "Who's your ideal customer? Describe them."
3. "Why do they choose your app over alternatives like personal color consultants?"
4. "What's your comfortable budget to learn what works?"
5. "Have you run ads before?"

**User:** [answers]

**Agent:** "I have enough to propose initial experiments. We can proceed now, or I can ask 3 more questions to refine. What do you prefer?"

---

**User:** "Proceed."

**Agent:** [Presents 3 experiment proposals with 30â‚¬ budget each, asks for approval]

**User:** "Yes, book them."

**Agent:** [Books campaigns, sets 7-day reminder]

---

[7 days later, proactive morning report]

**Agent:** "Here's your Week 1 performance report..."
