---
name: google-ads-ops
description: Use when the user wants to run Google Ads campaigns with arbitrage strategy, A/B test target groups and value propositions, or needs week-by-week campaign optimization with hypothesis-driven experiments.
---

# Google Ads Ops

## Overview
Run Google Ads campaigns with an arbitrage mindset: test hypotheses, measure Cost per Order (CpO), and iterate weekly. Focus on conversions, not vanity metrics. Each experiment tests ONE bold change so results are attributable.

## When to Use
- User mentions Google Ads, AdSense, or paid acquisition
- User wants to optimize conversion performance
- User has budget for experiments and wants measurable ROI
- User is starting from scratch OR has existing campaigns

## Phase 1: Socratic Discovery (Initialization)
**Goal:** Uncover the user's business model and arbitrage opportunity through questions.

### Opening Questions (ask 3â€“5)
1) **Business model:** â€œWhat do you sell and what does each conversion earn you?â€ (unit economics)
2) **Target audience:** â€œWho buys this? Describe your ideal customer.â€
   â†’ Followâ€‘up: â€œWhat do they care about most when making this decision?â€
3) **Value proposition:** â€œWhy do customers choose you over alternatives?â€
   â†’ Followâ€‘up: â€œIf you had to guess, what's the ONE thing that makes them convert?â€
4) **Budget:** â€œWhat's your comfortable loss limit to learn what works?â€ (default: 30â‚¬ per experiment)
5) **Experience:** â€œHave you run ads before? What worked or didn't?â€ (assume 0, ask kindly)
6) **Target group size:** â€œHow many potential customers exist for this? Local, national, global?â€

### Transition to Execution
After 3â€“5 questions, pause and offer:
> â€œI have enough to propose initial experiments. We can proceed now, or I can ask 3 more questions to refine the strategy. What do you prefer?â€

If user says **proceed** â†’ Move to Phase 2.

### Exit Criteria for Discovery
- âœ… Unit economics understood (what you pay vs. what you earn per conversion)
- âœ… Target audience hypothesis defined
- âœ… Value proposition hypothesis defined
- âœ… Experiment budget confirmed
- âœ… User has chosen to proceed

## Phase 2: Hypothesisâ€‘Driven Execution

### Budget Splitting Rule
**30â‚¬ per experiment.** Each experiment tests **ONE** hypothesis.

Example budget split (150â‚¬ total):
- Experiment A: 30â‚¬ â€” Test audience segment â€œfashionâ€‘conscious millennialsâ€
- Experiment B: 30â‚¬ â€” Test value prop â€œSave 2 hours per weekâ€
- Experiment C: 30â‚¬ â€” Test keyword cluster â€œpersonal color analysisâ€
- Experiment D: 30â‚¬ â€” Test landing page variant A
- Experiment E: 30â‚¬ â€” Test bidding strategy (maximize clicks vs conversions)

### The ONE Change Rule
Each experiment changes exactly **one** variable. Never mix changes.

| âŒ Bad (mixed changes) | âœ… Good (single change) |
|------------------------|-------------------------|
| New audience + new ad copy | New audience (same ad copy) |
| New keywords + higher bids | New keywords (same bids) |
| New landing page + new CTA | New landing page (same CTA) |

**Why:** If results change, we must know WHAT caused it.

### Campaign Proposal Format
Before booking, present to user:

**Proposed Experiment: [Name]**
- **Hypothesis:** â€œ[Audience/Value prop/Channel] will achieve [CpO target]â€
- **What we're testing:** [Single variable]
- **Control (baseline):** [What we're comparing against]
- **Budget:** 30â‚¬ over 7 days
- **Target metrics:**
  - Expected CpO: [X]â‚¬
  - Target conversions: [Y]
- **Campaign structure:**
  - Audience: [segment]
  - Keywords: [list]
  - Ad copy: [variant]
  - Landing page: [URL]

**Proceed? (yes/no/modify)**

User approval â†’ book campaign directly.

## Phase 3: Weekly Review Cycle

### Every 7 Days â€” Proactive report
```markdown
## Week [N] Performance Report

### Summary
- Spend: [X]â‚¬
- Conversions: [Y]
- Cost per Order (CpO): [Z]â‚¬
- Customer journey duration: [D] days

### Hypothesis Results
| Experiment | Hypothesis | Result | Verdict |
|------------|------------|--------|---------|
| A | [description] | CpO: [X]â‚¬ | âœ… Scale / âŒ Kill / ğŸ”„ Iterate |

### What We Learned
- [Insight 1]
- [Insight 2]

### Next Week's Experiments
Based on learnings, propose 2â€“3 new experiments.

Approve new experiments? (yes/no/discuss)

### Customer Journey Analysis
After first conversions, analyze and report:
- **Time from first click to conversion** (from Analytics data)
- **Touchpoints before conversion** (search â†’ ad â†’ landing page â†’ convert)
- **Implication for budget timing:** â€œConversions happen ~[X] days after click, so allocate budget for [X+2] day measurement windowâ€
```

### No Automatic Adjustments
**Rule:** Agent does NOT automatically change budgets or bids.

Workflow:
1) Measure â†’ 2) Learn â†’ 3) Report â†’ 4) Propose new hypotheses â†’ 5) User approves â†’ 6) Execute

## Phase 4: ROI Measurement

### Primary Metric: Cost per Order (CpO)
CpO = Total Spend / Number of Orders

Target: CpO < Customer Lifetime Value Ã— Profit Margin

### Secondary Metrics (for diagnostics)
| Metric | Use For |
|--------|---------|
| CTR | Ad relevance (low CTR â†’ test copy) |
| CPC | Competition level (high CPC â†’ test longâ€‘tail) |
| Conversion Rate | Landing page effectiveness |
| Quality Score | Keywordâ€‘ad relevance |

### Breakâ€‘Even Calculation
Ask user: â€œWhat's a customer worth to you?â€ â†’ Set CpO target at **50â€“70%** of that value for safety margin.

## Safety Rules
1) Never store credentials in repo â€” use env vars or local JSON only
2) Always confirm before booking campaigns, even small ones
3) Never exceed stated experiment budget without explicit approval
4) If API access fails â€” ask for credentials, don't guess
5) If data is insufficient â€” say so, don't fabricate insights

## Quick Reference
| Phase | Trigger | Action |
|-------|---------|--------|
| Discovery | User starts conversation | Ask 3â€“5 Socratic questions |
| Proposal | User ready to proceed | Present experiments, get approval |
| Execution | User approves | Book campaign, start 7â€‘day clock |
| Review | Every 7 days (proactive) | Generate report, propose next experiments |

## Common Mistakes
| Mistake | Fix |
|---------|-----|
| Testing multiple variables at once | Enforce ONE change per experiment |
| Skipping discovery questions | Always complete 3â€“5 questions minimum |
| Adjusting midâ€‘week | Let experiments run full 7 days |
| Ignoring customer journey timing | Analyze duration, adjust measurement window |
| Focusing on CTR instead of CpO | CTR is diagnostic, CpO is the goal |

## Example Dialogue
User: â€œI want to run Google Ads for my color analysis app.â€

Agent (Discovery):
1) â€œWhat does each conversion earn you? Is it a oneâ€‘time purchase or subscription?â€
2) â€œWhoâ€™s your ideal customer? Describe them.â€
3) â€œWhy do they choose your app over alternatives like personal color consultants?â€
4) â€œWhatâ€™s your comfortable budget to learn what works?â€
5) â€œHave you run ads before?â€

User: [answers]

Agent: â€œI have enough to propose initial experiments. We can proceed now, or I can ask 3 more questions to refine. What do you prefer?â€

---

User: â€œProceed.â€

Agent: [Presents 2â€“3 experiment proposals with 30â‚¬ budget each, asks for approval]

User: â€œYes, book them.â€

Agent: [Books campaigns, sets 7â€‘day reminder]

---

[7 days later, proactive morning report]
Agent: â€œHereâ€™s your Week 1 performance report...â€

## Resources
- API credentials & setup: `references/google_ads_setup.md`
- Business profile schema: `references/business_profile.schema.md`

## Safety / rules
- Never store credentials in repo.
- Always confirm budget caps and conversion goals before recommending spend changes.
- If data is missing or API access fails, ask for credentials or business profile.
